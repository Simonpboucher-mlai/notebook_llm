{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3be170-9042-40c3-9a70-c42c693d2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def generate_and_format_response(\n",
    "    provider,\n",
    "    model,\n",
    "    messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1500,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    min_tokens=None,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    "    random_seed=None,\n",
    "    response_format=None,\n",
    "    tools=None,\n",
    "    tool_choice=\"auto\",\n",
    "    safe_prompt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and formats a response from the specified LLM provider (OpenAI, Anthropic, or Mistral).\n",
    "    \n",
    "    Parameters:\n",
    "    - provider (str): The LLM provider to use (\"openai\", \"anthropic\", or \"mistral\").\n",
    "    - model (str): The model to use for text generation.\n",
    "    - messages (list): A list of messages in the format [{\"role\": \"user\", \"content\": \"Your prompt here\"}].\n",
    "    - temperature (float, optional): Controls randomness in the output (default: 0.7 for Anthropic & Mistral, 1.0 for OpenAI).\n",
    "    - max_tokens (int, optional): Max tokens to generate (default: 1500).\n",
    "    - top_p (float, optional): Nucleus sampling parameter for controlling diversity (default: 0.9).\n",
    "    - frequency_penalty (float, optional): Penalizes new tokens based on frequency (OpenAI only).\n",
    "    - presence_penalty (float, optional): Penalizes new tokens based on presence (OpenAI only).\n",
    "    - Additional options are specific to certain providers.\n",
    "    \n",
    "    Returns:\n",
    "    - formatted_text (str): The formatted assistant response in Markdown.\n",
    "    \"\"\"\n",
    "    \n",
    "    api_key = None\n",
    "    url = None\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Determine API key, endpoint, and headers based on provider\n",
    "    if provider.lower() == \"openai\":\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty\n",
    "        }\n",
    "\n",
    "    elif provider.lower() == \"anthropic\":\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        url = \"https://api.anthropic.com/v1/messages\"\n",
    "        headers[\"x-api-key\"] = api_key\n",
    "        headers[\"anthropic-version\"] = \"2023-06-01\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    elif provider.lower() == \"mistral\":\n",
    "        api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"stream\": stream,\n",
    "            \"tool_choice\": tool_choice,\n",
    "            \"safe_prompt\": safe_prompt\n",
    "        }\n",
    "        # Add optional parameters for Mistral\n",
    "        if max_tokens is not None:\n",
    "            payload[\"max_tokens\"] = max_tokens\n",
    "        if min_tokens is not None:\n",
    "            payload[\"min_tokens\"] = min_tokens\n",
    "        if stop is not None:\n",
    "            payload[\"stop\"] = stop\n",
    "        if random_seed is not None:\n",
    "            payload[\"random_seed\"] = random_seed\n",
    "        if response_format is not None:\n",
    "            payload[\"response_format\"] = response_format\n",
    "        if tools is not None:\n",
    "            payload[\"tools\"] = tools\n",
    "    else:\n",
    "        return \"Invalid provider. Please choose from 'openai', 'anthropic', or 'mistral'.\"\n",
    "\n",
    "    # Check if API key is available\n",
    "    if not api_key:\n",
    "        raise ValueError(f\"{provider.capitalize()} API key not found. Please set it in the .env file.\")\n",
    "    \n",
    "    try:\n",
    "        # Send the POST request to the provider's API\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        \n",
    "        # Raise an exception if the request was unsuccessful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # Extract the assistant's message and token information based on the provider\n",
    "        input_tokens = len(\" \".join(msg[\"content\"] for msg in messages).split())  # Estimate of input tokens\n",
    "        if provider.lower() == \"openai\":\n",
    "            assistant_message = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            output_tokens = response_data[\"usage\"][\"completion_tokens\"]\n",
    "\n",
    "        elif provider.lower() == \"anthropic\":\n",
    "            assistant_message = response_data.get(\"content\", [{}])[0].get(\"text\", \"No reply found.\")\n",
    "            output_tokens = len(assistant_message.split())  # Estimate of output tokens\n",
    "\n",
    "        elif provider.lower() == \"mistral\":\n",
    "            assistant_message = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No reply found.\")\n",
    "            output_tokens = len(assistant_message.split())  # Estimate of output tokens\n",
    "        \n",
    "        # Format the response including provider, model, and token information\n",
    "        formatted_text = (\n",
    "            f\"**Provider:** {provider.capitalize()} | **Model:** {model}  \\n\"\n",
    "            f\"**Tokens Used (Input/Output):** {input_tokens}/{output_tokens}  \\n\\n\"\n",
    "            f\"**Assistant:**\\n\\n{assistant_message}\\n\"\n",
    "        )\n",
    "        return formatted_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afcac43-6bea-44eb-8ef6-0b87a7612992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Openai | **Model:** gpt-3.5-turbo  \n",
       "**Tokens Used (Input/Output):** 16/251  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Quantum entanglement is a phenomenon in quantum mechanics where two or more particles become interconnected in such a way that the state of one particle is dependent on the state of the other, regardless of the distance between them. This means that measuring the state of one particle instantaneously determines the state of the other, even if they are light-years apart.\n",
       "\n",
       "This concept challenges classical notions of locality and realism because it violates the principle of locality, which states that an object is directly influenced only by its immediate surroundings. In the case of quantum entanglement, the state of one particle can be influenced by the state of another particle regardless of the distance between them, suggesting that there is some form of non-local connection between the particles.\n",
       "\n",
       "Additionally, quantum entanglement challenges the concept of realism, which posits that objects have well-defined properties independent of observation. In the case of entangled particles, their properties are not well-defined until they are measured, and the act of measuring one particle instantly determines the properties of the other particle, even if they were previously unknown.\n",
       "\n",
       "Overall, quantum entanglement challenges classical notions of locality and realism by demonstrating that particles can be interconnected in ways that defy our classical understanding of how objects interact with each other in the physical world.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the conversation messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement and how it challenges classical notions of locality and realism.\"}\n",
    "]\n",
    "\n",
    "# Generate and format the response from the specified provider\n",
    "formatted_response = generate_and_format_response(\n",
    "    provider=\"openai\",        # Change to \"anthropic\" or \"mistral\" as needed\n",
    "    model=\"gpt-3.5-turbo\",    # Adjust the model for the chosen provider\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1500,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Display the formatted response\n",
    "display(Markdown(formatted_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddf65d5-e2e2-431c-9f1b-895f4406e014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Anthropic | **Model:** claude-3-5-sonnet-20240620  \n",
       "**Tokens Used (Input/Output):** 16/585  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Quantum entanglement is a fundamental concept in quantum mechanics that describes a phenomenon where two or more particles become intrinsically linked, such that the quantum state of each particle cannot be described independently of the others, even when separated by large distances. This concept challenges our classical understanding of physics and has profound implications for our understanding of reality, particularly in terms of locality and realism.\n",
       "\n",
       "To understand how quantum entanglement challenges classical notions, let's break down the concept and its implications:\n",
       "\n",
       "1. Quantum Entanglement Basics:\n",
       "\n",
       "When particles become entangled, their quantum states are correlated in such a way that measuring the state of one particle instantaneously affects the state of its entangled partner(s), regardless of the distance between them. This correlation persists even when the particles are separated by vast distances, seemingly violating the principle of locality in classical physics.\n",
       "\n",
       "2. Challenging Locality:\n",
       "\n",
       "Locality in classical physics suggests that an object is influenced directly only by its immediate surroundings. The principle of locality states that an object can only be influenced by events in its immediate vicinity, and this influence cannot propagate faster than the speed of light.\n",
       "\n",
       "Quantum entanglement appears to violate this principle because the measurement of one entangled particle seems to instantaneously affect its partner, even if they are separated by large distances. This \"spooky action at a distance,\" as Einstein famously called it, suggests that information might be transmitted faster than the speed of light, which conflicts with special relativity.\n",
       "\n",
       "3. Challenging Realism:\n",
       "\n",
       "Classical realism assumes that physical properties of objects exist independently of observation. In other words, objects have definite properties whether we measure them or not.\n",
       "\n",
       "Quantum entanglement challenges this notion because the properties of entangled particles are not determined until they are measured. Before measurement, the particles exist in a superposition of states, and it's only upon observation that their properties become defined. This suggests that reality might not be as objective or independent of observation as classical physics assumes.\n",
       "\n",
       "4. The EPR Paradox and Bell's Theorem:\n",
       "\n",
       "The Einstein-Podolsky-Rosen (EPR) paradox highlighted the apparent conflict between quantum mechanics and local realism. They argued that quantum mechanics must be incomplete because it allowed for \"spooky action at a distance.\"\n",
       "\n",
       "John Bell later developed Bell's Theorem, which provided a way to test whether quantum mechanics could be explained by local hidden variable theories (which attempt to preserve local realism). Subsequent experiments have consistently violated Bell's inequalities, supporting quantum mechanics and suggesting that local realism must be abandoned.\n",
       "\n",
       "5. Implications and Interpretations:\n",
       "\n",
       "The challenges posed by quantum entanglement to our classical understanding of physics have led to various interpretations of quantum mechanics, such as the Copenhagen interpretation, many-worlds interpretation, and quantum decoherence. These interpretations attempt to reconcile quantum phenomena with our understanding of reality, often leading to profound philosophical questions about the nature of existence and consciousness.\n",
       "\n",
       "6. Practical Applications:\n",
       "\n",
       "Despite its counterintuitive nature, quantum entanglement has practical applications in fields such as quantum computing, quantum cryptography, and quantum teleportation. These technologies exploit the unique properties of entangled particles to perform tasks that are impossible with classical systems.\n",
       "\n",
       "In conclusion, quantum entanglement fundamentally challenges our classical notions of locality and realism by demonstrating non-local correlations and the dependence of reality on observation. This phenomenon forces us to reconsider our understanding of the nature of reality and the fundamental principles governing the universe. While it presents significant challenges to our intuitive understanding of physics, it also opens up new possibilities for technological advancements and deeper insights into the fabric of reality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the conversation messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement and how it challenges classical notions of locality and realism.\"}\n",
    "]\n",
    "\n",
    "# Generate and format the response for Anthropic\n",
    "formatted_response = generate_and_format_response(\n",
    "    provider=\"anthropic\",                # Specify the provider as Anthropic\n",
    "    model=\"claude-3-5-sonnet-20240620\",  # Replace with your specific Anthropic model\n",
    "    messages=messages,\n",
    "    temperature=0.7,                     # Adjust for creativity\n",
    "    max_tokens=1500,                     # Limit the length of the response\n",
    "    top_p=0.9                            # Control diversity\n",
    ")\n",
    "\n",
    "# Display the formatted response\n",
    "display(Markdown(formatted_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8dd60b-a083-4250-afd2-540e229d1641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Mistral | **Model:** mistral-small-latest  \n",
       "**Tokens Used (Input/Output):** 30/484  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Quantum entanglement is a phenomenon in quantum mechanics where two or more particles become correlated in such a way that the state of one particle cannot be described independently of the state of the other particles, even when the particles are separated by large distances. This correlation persists regardless of the distance between the particles, leading to what Einstein famously referred to as \"spooky action at a distance.\"\n",
       "\n",
       "### Challenges to Classical Notions\n",
       "\n",
       "1. **Locality**: In classical physics, the idea of locality holds that an object is directly influenced only by its immediate surroundings. However, in quantum entanglement, the state of one particle can instantly affect the state of another, regardless of the distance between them. This instantaneous correlation violates the principle of locality as understood in classical physics.\n",
       "\n",
       "2. **Realism**: Realism in physics assumes that particles have definite states independent of measurement. However, entanglement suggests that particles do not have definite states until measured. The act of measurement on one particle can instantly determine the state of another, which challenges the notion of realism.\n",
       "\n",
       "### Implications for Causality and Information Transfer\n",
       "\n",
       "1. **Causality**: Traditional notions of causality assume that effects follow their causes in a way that respects the speed of light. However, entanglement suggests that the state of one particle can instantaneously affect the state of another, seemingly violating the causal structure of spacetime. This is a profound challenge to our understanding of causality.\n",
       "\n",
       "2. **Information Transfer**: Entanglement also challenges our understanding of information transfer. In classical physics, information cannot be transmitted faster than the speed of light. However, the correlations in entangled particles seem to allow for instantaneous information transfer, which is often referred to as \"quantum teleportation.\" While this does not allow for the transmission of classical information faster than light (due to the no-communication theorem), it does suggest that information in a quantum system can be transferred in ways that are not possible in a classical system.\n",
       "\n",
       "### Experimental Evidence and Theoretical Implications\n",
       "\n",
       "- **Bell's Theorem**: John Bell formulated a set of inequalities (Bell's inequalities) that could be tested experimentally to distinguish between quantum mechanics and local hidden variable theories. Experiments have consistently violated these inequalities, supporting the predictions of quantum mechanics and challenging local realism.\n",
       "\n",
       "- **Quantum Computing and Cryptography**: Entanglement is a key resource in quantum computing and quantum cryptography. Quantum computers use entangled qubits to perform calculations that are infeasible for classical computers. Quantum cryptography, such as quantum key distribution (QKD), uses entanglement to ensure secure communication.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Quantum entanglement challenges our classical intuitions about locality and realism, and it has profound implications for our understanding of causality and information transfer. While it does not allow for faster-than-light communication, it does enable phenomena like quantum teleportation and quantum computing that are not possible in classical systems. The study of entanglement continues to be a vibrant area of research in quantum mechanics and quantum information theory.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the conversation messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement and how it challenges classical notions of locality and realism. What are the implications of entanglement for our understanding of causality and information transfer?\"}\n",
    "]\n",
    "\n",
    "# Generate and format the response for Mistral\n",
    "formatted_response = generate_and_format_response(\n",
    "    provider=\"mistral\",                  # Specify the provider as Mistral\n",
    "    model=\"mistral-small-latest\",        # Replace with your specific Mistral model\n",
    "    messages=messages,\n",
    "    temperature=0.7,                     # Adjust for creativity\n",
    "    max_tokens=2000,                     # Limit the length of the response\n",
    "    top_p=0.9                            # Control diversity\n",
    ")\n",
    "\n",
    "# Display the formatted response\n",
    "display(Markdown(formatted_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f23680d-a6a4-4def-bb73-b2d1d28aa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, provider, model, api_key=None, temperature=0.7, max_tokens=1500, top_p=0.9, frequency_penalty=0.0, presence_penalty=0.0, min_tokens=None, stream=False, stop=None, random_seed=None, response_format=None, tools=None, tool_choice=\"auto\", safe_prompt=False):\n",
    "        \"\"\"\n",
    "        Initialize the ChatBot with API key, provider, and parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - provider (str): The LLM provider to use (\"openai\", \"anthropic\", or \"mistral\").\n",
    "        - model (str): The model to use for text generation.\n",
    "        - api_key (str): The API key for the chosen provider.\n",
    "        - temperature (float): Controls randomness in the output.\n",
    "        - max_tokens (int): The maximum number of tokens to generate in the completion.\n",
    "        - Additional parameters are specific to certain providers.\n",
    "        \"\"\"\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.api_key = api_key or os.getenv(f\"{self.provider.upper()}_API_KEY\")\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.top_p = top_p\n",
    "        self.frequency_penalty = frequency_penalty\n",
    "        self.presence_penalty = presence_penalty\n",
    "        self.min_tokens = min_tokens\n",
    "        self.stream = stream\n",
    "        self.stop = stop\n",
    "        self.random_seed = random_seed\n",
    "        self.response_format = response_format\n",
    "        self.tools = tools\n",
    "        self.tool_choice = tool_choice\n",
    "        self.safe_prompt = safe_prompt\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"\n",
    "        Add a message to the conversation history.\n",
    "        \n",
    "        Parameters:\n",
    "        - role (str): The role of the sender (\"user\" or \"assistant\").\n",
    "        - content (str): The content of the message.\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def _call_api(self):\n",
    "        \"\"\"\n",
    "        Internal method to call the appropriate API based on provider and generate a response.\n",
    "        \n",
    "        Returns:\n",
    "        - response (dict): The API response as a dictionary.\n",
    "        \"\"\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        if self.provider == \"openai\":\n",
    "            url = \"https://api.openai.com/v1/chat/completions\"\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": self.conversation_history,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"top_p\": self.top_p,\n",
    "                \"frequency_penalty\": self.frequency_penalty,\n",
    "                \"presence_penalty\": self.presence_penalty\n",
    "            }\n",
    "\n",
    "        elif self.provider == \"anthropic\":\n",
    "            url = \"https://api.anthropic.com/v1/messages\"\n",
    "            headers[\"x-api-key\"] = self.api_key\n",
    "            headers[\"anthropic-version\"] = \"2023-06-01\"\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_tokens\": self.max_tokens,\n",
    "                \"top_p\": self.top_p,\n",
    "                \"messages\": self.conversation_history\n",
    "            }\n",
    "\n",
    "        elif self.provider == \"mistral\":\n",
    "            url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": self.conversation_history,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"top_p\": self.top_p,\n",
    "                \"stream\": self.stream,\n",
    "                \"tool_choice\": self.tool_choice,\n",
    "                \"safe_prompt\": self.safe_prompt\n",
    "            }\n",
    "            # Add optional parameters for Mistral\n",
    "            if self.max_tokens is not None:\n",
    "                payload[\"max_tokens\"] = self.max_tokens\n",
    "            if self.min_tokens is not None:\n",
    "                payload[\"min_tokens\"] = self.min_tokens\n",
    "            if self.stop is not None:\n",
    "                payload[\"stop\"] = self.stop\n",
    "            if self.random_seed is not None:\n",
    "                payload[\"random_seed\"] = self.random_seed\n",
    "            if self.response_format is not None:\n",
    "                payload[\"response_format\"] = self.response_format\n",
    "            if self.tools is not None:\n",
    "                payload[\"tools\"] = self.tools\n",
    "        else:\n",
    "            raise ValueError(\"Invalid provider. Choose from 'openai', 'anthropic', or 'mistral'.\")\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_response(self, user_input):\n",
    "        \"\"\"\n",
    "        Get a response from the selected provider's API based on the user input.\n",
    "        \n",
    "        Parameters:\n",
    "        - user_input (str): The user's input message.\n",
    "        \n",
    "        Returns:\n",
    "        - formatted_response (str): The assistant's formatted response in Markdown.\n",
    "        \"\"\"\n",
    "        # Add user input to conversation history\n",
    "        self.add_message(\"user\", user_input)\n",
    "\n",
    "        # Call the API to get a response\n",
    "        response = self._call_api()\n",
    "\n",
    "        if response:\n",
    "            # Extract the assistant's reply based on provider\n",
    "            if self.provider == \"openai\":\n",
    "                assistant_reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                token_usage = response[\"usage\"][\"completion_tokens\"]\n",
    "\n",
    "            elif self.provider == \"anthropic\":\n",
    "                assistant_reply = response.get(\"content\", [{}])[0].get(\"text\", \"No reply found.\")\n",
    "                token_usage = len(assistant_reply.split())  # Estimated tokens for Anthropic\n",
    "\n",
    "            elif self.provider == \"mistral\":\n",
    "                assistant_reply = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No reply found.\")\n",
    "                token_usage = len(assistant_reply.split())  # Estimated tokens for Mistral\n",
    "\n",
    "            # Add assistant's reply to conversation history\n",
    "            self.add_message(\"assistant\", assistant_reply)\n",
    "\n",
    "            # Format the response using Markdown\n",
    "            formatted_response = (\n",
    "                f\"**Provider:** {self.provider.capitalize()} | **Model:** {self.model}  \\n\"\n",
    "                f\"**Tokens Used:** {token_usage}  \\n\\n\"\n",
    "                f\"**Assistant:**\\n\\n{assistant_reply}\\n\"\n",
    "            )\n",
    "            return formatted_response\n",
    "        else:\n",
    "            return \"Sorry, I couldn't generate a response.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bd4572-1440-424a-9ca8-70320e2dd087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Openai | **Model:** gpt-4o  \n",
       "**Tokens Used:** 326  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Certainly! Here are five dinner ideas for you to consider this week:\n",
       "\n",
       "1. **Monday: Lemon Herb Grilled Chicken with Quinoa Salad**\n",
       "   - Marinate chicken breasts in olive oil, lemon juice, garlic, and herbs like thyme and rosemary. Grill until cooked through.\n",
       "   - Serve with a quinoa salad tossed with cherry tomatoes, cucumber, red onion, feta cheese, and a lemon vinaigrette.\n",
       "\n",
       "2. **Tuesday: Spaghetti Aglio e Olio**\n",
       "   - Cook spaghetti and toss it with sautéed garlic in olive oil, red pepper flakes, and parsley.\n",
       "   - Serve with freshly grated parmesan and a side of garlic bread.\n",
       "\n",
       "3. **Wednesday: Beef Stir-Fry with Broccoli and Bell Peppers**\n",
       "   - Sauté sliced beef with garlic, ginger, and a mix of soy sauce and sesame oil.\n",
       "   - Add broccoli and bell peppers and stir-fry until tender. Serve over steamed rice.\n",
       "\n",
       "4. **Thursday: Baked Salmon with Roasted Vegetables**\n",
       "   - Season salmon fillets with salt, pepper, and dill. Bake in the oven until just cooked.\n",
       "   - Pair with a mix of roasted potatoes, carrots, and Brussel sprouts, drizzled with olive oil and seasoned with rosemary.\n",
       "\n",
       "5. **Friday: Vegetarian Tacos with Black Beans and Avocado**\n",
       "   - Fill corn tortillas with a mixture of seasoned black beans, diced avocado, and chopped tomatoes.\n",
       "   - Top with shredded lettuce, cheese, sour cream, and freshly squeezed lime juice.\n",
       "\n",
       "These meals are not only easy to prepare but also nutritious and full of flavor. Enjoy your dinners!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Initialize ChatBot with OpenAI, Anthropic, or Mistral\n",
    "\n",
    "# Replace 'openai' with 'anthropic' or 'mistral' as needed\n",
    "bot = ChatBot(provider=\"openai\", model=\"gpt-4o\", temperature=1.0, max_tokens=2000, top_p=1.0)\n",
    "\n",
    "# Get a response from the chatbot\n",
    "user_input = \"Can you suggest 5 dinner ideas for this week?\"\n",
    "response = bot.get_response(user_input)\n",
    "\n",
    "# Display the formatted response as Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ebd96c-dd5b-4ba2-aa73-e29bdfebdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Anthropic | **Model:** claude-3-5-sonnet-20240620  \n",
       "**Tokens Used:** 614  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Quantum entanglement is a fascinating phenomenon in quantum mechanics that has profound implications for our understanding of the nature of reality, causality, and information. To explain this concept and its implications, let's break it down into several key points:\n",
       "\n",
       "1. Definition of Quantum Entanglement:\n",
       "Quantum entanglement occurs when two or more particles become correlated in such a way that the quantum state of each particle cannot be described independently of the others, even when the particles are separated by a large distance. In other words, the particles become intrinsically linked, and their properties are fundamentally connected.\n",
       "\n",
       "2. Einstein-Podolsky-Rosen (EPR) Paradox:\n",
       "The concept of quantum entanglement was first highlighted by Einstein, Podolsky, and Rosen in 1935 as a challenge to the completeness of quantum mechanics. They argued that entanglement implied the existence of \"spooky action at a distance,\" which seemed to violate the principle of locality in classical physics.\n",
       "\n",
       "3. Bell's Theorem:\n",
       "In 1964, John Stewart Bell proposed a way to test whether quantum entanglement could be explained by local hidden variables. His theorem showed that the correlations predicted by quantum mechanics are stronger than what could be explained by any local hidden variable theory, leading to experimental tests that have consistently supported quantum mechanics.\n",
       "\n",
       "4. Experimental Confirmation:\n",
       "Numerous experiments, including those by Alain Aspect in the 1980s and more recent ones, have demonstrated the reality of quantum entanglement, confirming that entangled particles can indeed influence each other instantaneously, regardless of their separation in space.\n",
       "\n",
       "Implications for Causality:\n",
       "\n",
       "1. Non-locality:\n",
       "Quantum entanglement challenges our classical notion of locality, which states that objects can only be influenced by their immediate surroundings. Entanglement suggests that there can be instantaneous correlations between distant particles, seemingly violating the speed of light limit for information transfer.\n",
       "\n",
       "2. Faster-than-light Communication?:\n",
       "While entanglement allows for instantaneous correlations, it's important to note that it cannot be used for faster-than-light communication. The outcomes of measurements on entangled particles are random, and no information can be transmitted intentionally through this process.\n",
       "\n",
       "3. Quantum Causality:\n",
       "Entanglement forces us to reconsider our understanding of causality. In the quantum world, cause and effect relationships become blurred, and the idea of a definite sequence of events becomes less clear-cut.\n",
       "\n",
       "4. Quantum Information:\n",
       "Entanglement is a crucial resource in quantum information theory and quantum computing. It allows for phenomena like quantum teleportation and super-dense coding, which have no classical analogues.\n",
       "\n",
       "5. Holistic Nature of Reality:\n",
       "Entanglement suggests that the universe may be fundamentally interconnected at a quantum level, challenging reductionist views of reality and pointing towards a more holistic understanding of nature.\n",
       "\n",
       "6. Time and Entanglement:\n",
       "Some interpretations of quantum mechanics, such as the transactional interpretation, suggest that entanglement might involve connections not just across space but also across time, further complicating our understanding of causality.\n",
       "\n",
       "7. Measurement Problem:\n",
       "Entanglement is closely related to the measurement problem in quantum mechanics. The act of measuring one entangled particle instantaneously affects the state of its partner, raising questions about the nature of measurement and the role of consciousness in quantum phenomena.\n",
       "\n",
       "8. Quantum Foundations:\n",
       "The phenomenon of entanglement continues to be a central topic in discussions about the foundations of quantum mechanics and the nature of reality itself. It has led to various interpretations of quantum mechanics, each with different implications for causality and the nature of the quantum world.\n",
       "\n",
       "In conclusion, quantum entanglement challenges our classical notions of causality, locality, and the nature of information. It suggests a universe that is more interconnected and less deterministic than classical physics would have us believe. While the full implications of entanglement are still being explored, it has already revolutionized our understanding of the quantum world and opened up new possibilities in quantum technologies.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize ChatBot with Anthropic\n",
    "bot = ChatBot(\n",
    "    provider=\"anthropic\", \n",
    "    model=\"claude-3-5-sonnet-20240620\",  # Replace with your specific Anthropic model if needed\n",
    "    temperature=0.7, \n",
    "    max_tokens=1500, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Get a response from the chatbot\n",
    "user_input = \"Explain the concept of quantum entanglement and its implications for understanding causality.\"\n",
    "response = bot.get_response(user_input)\n",
    "\n",
    "# Display the formatted response as Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb3b215-8cb1-4774-8761-543f9226b272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Mistral | **Model:** mistral-small-latest  \n",
       "**Tokens Used:** 455  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Quantum entanglement is a phenomenon in quantum physics where two or more particles become correlated in such a way that the state of one particle cannot be described independently of the state of the other particles, even when they are separated by large distances. This correlation persists regardless of the distance between the particles, which led Albert Einstein to famously refer to it as \"spooky action at a distance.\"\n",
       "\n",
       "### Key Concepts of Quantum Entanglement:\n",
       "\n",
       "1. **Superposition**: Quantum particles can exist in multiple states simultaneously until they are measured. This is known as superposition.\n",
       "\n",
       "2. **Correlation**: When particles become entangled, the state of one particle is instantly correlated with the state of the other(s). For example, if you measure an entangled particle and find it in a specific state (e.g., spin up), the other entangled particle will immediately be in the corresponding state (e.g., spin down), no matter the distance between them.\n",
       "\n",
       "3. **Measurement**: The act of measuring one particle's state instantaneously determines the state of the other particle, even if they are light-years apart.\n",
       "\n",
       "### Implications for Understanding Causality:\n",
       "\n",
       "1. **Non-locality**: Quantum entanglement challenges our classical understanding of causality, which assumes that an effect cannot precede its cause and that information cannot travel faster than the speed of light. The instantaneous correlation between entangled particles suggests that there might be a form of instantaneous action at a distance, which is non-local.\n",
       "\n",
       "2. **Bell's Theorem**: John Bell formulated inequalities (Bell's inequalities) that could be tested experimentally to distinguish between quantum mechanics and classical theories. Experimental results consistently support quantum mechanics, suggesting that local hidden variables cannot explain the correlations observed in entangled systems.\n",
       "\n",
       "3. **Einstein-Podolsky-Rosen (EPR) Paradox**: The EPR paradox highlighted the tension between quantum mechanics and the principle of locality. Entanglement implies that either the principle of locality is violated, or there are hidden variables that we do not yet understand.\n",
       "\n",
       "4. **Quantum Information Theory**: Entanglement is a crucial resource in quantum information science, enabling phenomena like quantum teleportation and quantum computing. It suggests that information can be processed and transmitted in ways that are fundamentally different from classical systems.\n",
       "\n",
       "5. **Foundations of Quantum Mechanics**: The implications of entanglement for causality are still a subject of debate and research. Some interpretations of quantum mechanics, such as the many-worlds interpretation and the transactional interpretation, offer different perspectives on how to reconcile entanglement with our intuitive notions of causality.\n",
       "\n",
       "### Conclusion:\n",
       "\n",
       "Quantum entanglement has profound implications for our understanding of causality. It challenges the classical notion of locality and suggests that there might be deeper, non-local aspects of reality that we do not yet fully comprehend. Ongoing research aims to further elucidate these phenomena and potentially integrate them into a more comprehensive understanding of the universe.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize ChatBot with Mistral\n",
    "bot = ChatBot(\n",
    "    provider=\"mistral\", \n",
    "    model=\"mistral-small-latest\",  # Replace with your specific Mistral model if needed\n",
    "    temperature=0.7, \n",
    "    max_tokens=1500, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Get a response from the chatbot\n",
    "user_input = \"Explain the concept of quantum entanglement and its implications for understanding causality.\"\n",
    "response = bot.get_response(user_input)\n",
    "\n",
    "# Display the formatted response as Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b528e72-813f-4e52-b520-640115783b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "def retrieve_relevant_docs(query, documents):\n",
    "    \"\"\"\n",
    "    Simple keyword-based function to retrieve relevant documents.\n",
    "    \"\"\"\n",
    "    relevant_docs = []\n",
    "    for doc in documents:\n",
    "        if any(keyword.lower() in doc.lower() for keyword in query.split()):\n",
    "            relevant_docs.append(doc)\n",
    "    return relevant_docs\n",
    "\n",
    "def generate_and_format_response(\n",
    "    provider,\n",
    "    model,\n",
    "    messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1500,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    min_tokens=None,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    "    random_seed=None,\n",
    "    response_format=None,\n",
    "    tools=None,\n",
    "    tool_choice=\"auto\",\n",
    "    safe_prompt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates and formats a response from the specified LLM provider (OpenAI, Anthropic, or Mistral) in Markdown format.\n",
    "    \"\"\"\n",
    "    api_key = None\n",
    "    url = None\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Determine API key, endpoint, and headers based on provider\n",
    "    if provider.lower() == \"openai\":\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty\n",
    "        }\n",
    "\n",
    "    elif provider.lower() == \"anthropic\":\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        url = \"https://api.anthropic.com/v1/messages\"\n",
    "        headers[\"x-api-key\"] = api_key\n",
    "        headers[\"anthropic-version\"] = \"2023-06-01\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    elif provider.lower() == \"mistral\":\n",
    "        api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"stream\": stream,\n",
    "            \"tool_choice\": tool_choice,\n",
    "            \"safe_prompt\": safe_prompt\n",
    "        }\n",
    "        if max_tokens is not None:\n",
    "            payload[\"max_tokens\"] = max_tokens\n",
    "        if min_tokens is not None:\n",
    "            payload[\"min_tokens\"] = min_tokens\n",
    "        if stop is not None:\n",
    "            payload[\"stop\"] = stop\n",
    "        if random_seed is not None:\n",
    "            payload[\"random_seed\"] = random_seed\n",
    "        if response_format is not None:\n",
    "            payload[\"response_format\"] = response_format\n",
    "        if tools is not None:\n",
    "            payload[\"tools\"] = tools\n",
    "    else:\n",
    "        return \"Invalid provider. Please choose from 'openai', 'anthropic', or 'mistral'.\"\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(f\"{provider.capitalize()} API key not found. Please set it in the .env file.\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        \n",
    "        input_tokens = len(\" \".join(msg[\"content\"] for msg in messages).split())\n",
    "        if provider.lower() == \"openai\":\n",
    "            assistant_message = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            output_tokens = response_data[\"usage\"][\"completion_tokens\"]\n",
    "\n",
    "        elif provider.lower() == \"anthropic\":\n",
    "            assistant_message = response_data.get(\"content\", [{}])[0].get(\"text\", \"No reply found.\")\n",
    "            output_tokens = len(assistant_message.split())\n",
    "\n",
    "        elif provider.lower() == \"mistral\":\n",
    "            assistant_message = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No reply found.\")\n",
    "            output_tokens = len(assistant_message.split())\n",
    "        \n",
    "        formatted_text = (\n",
    "            f\"**Provider:** {provider.capitalize()} | **Model:** {model}  \\n\"\n",
    "            f\"**Tokens Used (Input/Output):** {input_tokens}/{output_tokens}  \\n\\n\"\n",
    "            f\"**Assistant:**\\n\\n{assistant_message}\\n\"\n",
    "        )\n",
    "        return formatted_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def generate_response_with_rag(\n",
    "    provider,\n",
    "    model,\n",
    "    conversation_history,\n",
    "    current_message,\n",
    "    documents,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1500,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    min_tokens=None,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    "    random_seed=None,\n",
    "    response_format=None,\n",
    "    tools=None,\n",
    "    tool_choice=\"auto\",\n",
    "    safe_prompt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper function to perform RAG (Retrieval-Augmented Generation) by retrieving relevant documents,\n",
    "    combining them into a context, and generating a response with the specified provider.\n",
    "    \"\"\"\n",
    "    relevant_docs = retrieve_relevant_docs(current_message, documents)\n",
    "    context = \"\\n\\n\".join(relevant_docs)\n",
    "    augmented_message = f\"Context: {context}\\n\\n{current_message}\"\n",
    "    messages = conversation_history + [{\"role\": \"user\", \"content\": augmented_message}]\n",
    "\n",
    "    response = generate_and_format_response(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        min_tokens=min_tokens,\n",
    "        stream=stream,\n",
    "        stop=stop,\n",
    "        random_seed=random_seed,\n",
    "        response_format=response_format,\n",
    "        tools=tools,\n",
    "        tool_choice=tool_choice,\n",
    "        safe_prompt=safe_prompt\n",
    "    )\n",
    "\n",
    "    # Display formatted response as Markdown\n",
    "    display(Markdown(response))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb1a7003-bd17-459e-806e-54e9de6af713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Mistral | **Model:** mistral-small-latest  \n",
       "**Tokens Used (Input/Output):** 90/28  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "The debt-to-equity ratio of ABC Corp. is given as 0.3 in the context provided. This means that for every dollar of equity, the company has $0.30 of debt.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define conversation history and current message\n",
    "conversation_history = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "]\n",
    "current_message = \"What is the debt-to-equity ratio of ABC Corp?\"\n",
    "\n",
    "# Sample documents for retrieval\n",
    "documents = [\n",
    "    \"ABC Corp. reported a revenue of 50 million for Q2 2024, a 10 percent increase from Q1 2024.\",\n",
    "    \"ABC Corp. has a current debt-to-equity ratio of 0.3, indicating that the company has a low level of debt compared to its equity.\",\n",
    "    \"The market capitalization of ABC Corp. is currently 300 million.\",\n",
    "    \"In Q2 2024, ABC Corp. announced a dividend of 0.50 per share.\",\n",
    "    \"ABC Corp.'s gross profit margin for Q2 2024 was 40 percent.\"\n",
    "]\n",
    "\n",
    "# Generate response with RAG using Mistral as the provider\n",
    "formatted_response = generate_response_with_rag(\n",
    "    provider=\"mistral\", \n",
    "    model=\"mistral-small-latest\",  # Replace with your specific Mistral model\n",
    "    conversation_history=conversation_history, \n",
    "    current_message=current_message, \n",
    "    documents=documents, \n",
    "    temperature=0.7, \n",
    "    max_tokens=256, \n",
    "    top_p=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4feafb06-57fb-4d16-a3af-384c4e162099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Openai | **Model:** gpt-4  \n",
       "**Tokens Used (Input/Output):** 90/16  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "The debt-to-equity ratio of ABC Corp. is 0.3.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define conversation history and current message\n",
    "conversation_history = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "]\n",
    "current_message = \"What is the debt-to-equity ratio of ABC Corp?\"\n",
    "\n",
    "# Sample documents for retrieval\n",
    "documents = [\n",
    "    \"ABC Corp. reported a revenue of 50 million for Q2 2024, a 10 percent increase from Q1 2024.\",\n",
    "    \"ABC Corp. has a current debt-to-equity ratio of 0.3, indicating that the company has a low level of debt compared to its equity.\",\n",
    "    \"The market capitalization of ABC Corp. is currently 300 million.\",\n",
    "    \"In Q2 2024, ABC Corp. announced a dividend of 0.50 per share.\",\n",
    "    \"ABC Corp.'s gross profit margin for Q2 2024 was 40 percent.\"\n",
    "]\n",
    "\n",
    "# Generate response with RAG using OpenAI as the provider\n",
    "formatted_response = generate_response_with_rag(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4\",  # Replace with your specific OpenAI model\n",
    "    conversation_history=conversation_history, \n",
    "    current_message=current_message, \n",
    "    documents=documents, \n",
    "    temperature=0.7, \n",
    "    max_tokens=256, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# The formatted response is displayed directly in Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38af1ce3-187a-4bb2-ada6-9d7dc0170418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Provider:** Anthropic | **Model:** claude-3-5-sonnet-20240620  \n",
       "**Tokens Used (Input/Output):** 90/93  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Based on the information provided in the context, the debt-to-equity ratio of ABC Corp. is 0.3.\n",
       "\n",
       "The debt-to-equity ratio is a financial metric that compares a company's total debt to its total shareholders' equity. It's used to evaluate a company's financial leverage and risk. A lower ratio generally indicates that a company is using less debt financing relative to equity financing.\n",
       "\n",
       "In this case, ABC Corp.'s debt-to-equity ratio of 0.3 is considered relatively low, suggesting that the company has a conservative capital structure with a lower level of debt compared to its equity.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define conversation history and current message\n",
    "conversation_history = [\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "]\n",
    "current_message = \"What is the debt-to-equity ratio of ABC Corp?\"\n",
    "\n",
    "# Sample documents for retrieval\n",
    "documents = [\n",
    "    \"ABC Corp. reported a revenue of 50 million for Q2 2024, a 10 percent increase from Q1 2024.\",\n",
    "    \"ABC Corp. has a current debt-to-equity ratio of 0.3, indicating that the company has a low level of debt compared to its equity.\",\n",
    "    \"The market capitalization of ABC Corp. is currently 300 million.\",\n",
    "    \"In Q2 2024, ABC Corp. announced a dividend of 0.50 per share.\",\n",
    "    \"ABC Corp.'s gross profit margin for Q2 2024 was 40 percent.\"\n",
    "]\n",
    "\n",
    "# Generate response with RAG using Anthropic as the provider\n",
    "formatted_response = generate_response_with_rag(\n",
    "    provider=\"anthropic\", \n",
    "    model=\"claude-3-5-sonnet-20240620\",  # Replace with your specific Anthropic model\n",
    "    conversation_history=conversation_history, \n",
    "    current_message=current_message, \n",
    "    documents=documents, \n",
    "    temperature=0.7, \n",
    "    max_tokens=256, \n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26631224-1064-4385-803d-4efad5421087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Maximum token limit for chunk processing\n",
    "MAX_TOKENS = 8192 // 2  # Divide to ensure each chunk is within model limits\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, max_tokens):\n",
    "    \"\"\"\n",
    "    Splits text into chunks to fit within the token limit.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length >= max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunk(provider, api_key, chunk, model=\"gpt-4o-mini\", temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Summarizes a single chunk of text using the specified provider.\n",
    "    \"\"\"\n",
    "    prompt = f\"Summarize the following text in one sentence:\\n\\n{chunk}\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    return generate_and_format_response(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "def summarize_pdf_with_provider(provider, pdf_path, model=\"gpt-4o-mini\", temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Summarize chunks privide.\n",
    "    \"\"\"\n",
    "    # Convert PDF to text\n",
    "    text = pdf_to_text(pdf_path)\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = split_text_into_chunks(text, MAX_TOKENS)\n",
    "\n",
    "    # Summarize each chunk\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "        summary = summarize_chunk(provider, os.getenv(f\"{provider.upper()}_API_KEY\"), chunk, model=model, temperature=temperature, max_tokens=max_tokens)\n",
    "        summaries.append(summary)\n",
    "        print(f\"Chunk {i+1} summarized.\")\n",
    "\n",
    "    # Combine chunk summaries to generate final summary\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    final_summary_prompt = f\"Summarize the following text in a concise summary:\\n\\n{combined_summary}\"\n",
    "    final_summary = generate_and_format_response(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": final_summary_prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    # Display the final summary in Markdown format\n",
    "    display(Markdown(final_summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bd65944-8a79-480f-8d28-55eb4d1428d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/7\n",
      "Chunk 1 summarized.\n",
      "Processing chunk 2/7\n",
      "Chunk 2 summarized.\n",
      "Processing chunk 3/7\n",
      "Chunk 3 summarized.\n",
      "Processing chunk 4/7\n",
      "Chunk 4 summarized.\n",
      "Processing chunk 5/7\n",
      "Chunk 5 summarized.\n",
      "Processing chunk 6/7\n",
      "Chunk 6 summarized.\n",
      "Processing chunk 7/7\n",
      "Chunk 7 summarized.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Provider:** Anthropic | **Model:** claude-3-5-sonnet-20240620  \n",
       "**Tokens Used (Input/Output):** 359/104  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "Here is a concise summary of the key points from the provided text excerpts:\n",
       "\n",
       "The Bank of Canada's restrictive monetary policy has successfully reduced inflation but may now be too aggressive, potentially harming economic growth and employment. Canada is facing labor market challenges including declining employment and weak job creation. Interest rate cuts are expected due to lower inflation pressures and a cooling labor market. The text also includes economic forecasts for Canadian provinces/territories and national indicators. Legal disclaimers emphasize the report is not independent research or investment advice. Detailed regulatory information is provided about National Bank of Canada's financial services in various jurisdictions.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "pdf_path = \"sample_document.pdf\"  # Replace with the path to your PDF\n",
    "summarize_pdf_with_provider(provider=\"anthropic\", pdf_path=pdf_path, model=\"claude-3-5-sonnet-20240620\", temperature=0.7, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9286f78-6c7c-4db5-9d39-b2ed29b50274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/7\n",
      "Chunk 1 summarized.\n",
      "Processing chunk 2/7\n",
      "Chunk 2 summarized.\n",
      "Processing chunk 3/7\n",
      "Chunk 3 summarized.\n",
      "Processing chunk 4/7\n",
      "Chunk 4 summarized.\n",
      "Processing chunk 5/7\n",
      "Chunk 5 summarized.\n",
      "Processing chunk 6/7\n",
      "Chunk 6 summarized.\n",
      "Processing chunk 7/7\n",
      "Chunk 7 summarized.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Provider:** Openai | **Model:** gpt-4o-mini  \n",
       "**Tokens Used (Input/Output):** 352/83  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "The text discusses the need for the Bank of Canada to reconsider its restrictive monetary policy due to high interest rates negatively impacting economic growth and the labor market, despite lower inflation. It highlights stagnation in the Canadian labor market, differing inflationary pressures between the US and Canada, and provides various economic forecasts and financial data from National Bank Financial, emphasizing the document's nature as a marketing tool for professional investors without regulatory oversight.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "pdf_path = \"sample_document.pdf\"  # Replace with the path to your PDF\n",
    "summarize_pdf_with_provider(provider=\"openai\", pdf_path=pdf_path, model=\"gpt-4o-mini\", temperature=0.7, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7be1714e-d8d1-4b57-a5a8-fd3f592f42ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/7\n",
      "Chunk 1 summarized.\n",
      "Processing chunk 2/7\n",
      "Chunk 2 summarized.\n",
      "Processing chunk 3/7\n",
      "Chunk 3 summarized.\n",
      "Processing chunk 4/7\n",
      "Chunk 4 summarized.\n",
      "Processing chunk 5/7\n",
      "Chunk 5 summarized.\n",
      "Processing chunk 6/7\n",
      "Chunk 6 summarized.\n",
      "Processing chunk 7/7\n",
      "Chunk 7 summarized.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Provider:** Mistral | **Model:** mistral-small-latest  \n",
       "**Tokens Used (Input/Output):** 291/68  \n",
       "\n",
       "**Assistant:**\n",
       "\n",
       "The text discusses the Bank of Canada's monetary policy, recommending easing due to controlled inflation but weak economic growth and labor market. It also highlights Canada's labor market struggles, divergent monetary policies with the US, and varying inflation pressures. Economic forecasts for Canada from 2021 to 2025 are provided, along with a marketing document disclaimer from National Bank Financial Inc. regarding its investment services and report distribution restrictions.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "pdf_path = \"sample_document.pdf\"  # Replace with the path to your PDF\n",
    "summarize_pdf_with_provider(provider=\"mistral\", pdf_path=pdf_path, model=\"mistral-small-latest\", temperature=0.7, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69667e6-ec09-4ef1-ac61-0d61c59d402d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a498a-9b9b-4ace-b7d2-f882784847c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
